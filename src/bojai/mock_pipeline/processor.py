from abc import ABC, abstractmethod
from torch.utils.data import Dataset

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ProcessorManager: Chooses the right data processor and sets up train/eval sets
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'''
ğŸ“¦ The ProcessorManager is the entry point of the data preprocessing stage.
It handles:
    - Which data processor to use (based on your task type or setup)
    - How the dataset is split into training and evaluation sets
    - Passing the model, tokenizer, and device into the processor

ğŸ§  You only need to create ONE processor class that inherits from `Processor`.
This class will be instantiated 3 times:
    1. As the full processor (loads and splits data)
    2. As the train dataset (wraps the train portion)
    3. As the eval dataset (wraps the eval portion)

ğŸ” Once youâ€™ve implemented your processor, plug it into the `decide_which_processor` method below.
'''
class ProcessorManager():
    def __init__(self, data_dir, division, model, device, tokenizer, task_type):
        self.data_dir = data_dir
        self.division = division
        self.tokenizer = tokenizer
        self.model = model
        self.device = device
        self.processor = None
        self.decide_which_processor(task_type)

    def decide_which_processor(self, task_type):
        # Replace this with logic to choose between multiple processors if needed
        self.processor = YourDataProcessor(self.data_dir, self.division, self.model, self.device, self.tokenizer)
        self.train = YourDataProcessor(
            None, [0,1], self.model, self.device, self.tokenizer,
            is_main=False,
            inputs=self.processor.inputs_train,
            outputs=self.processor.outputs_train
        )
        self.eval = YourDataProcessor(
            None, [1,0], self.model, self.device, self.tokenizer,
            is_main=False,
            inputs=self.processor.inputs_eval,
            outputs=self.processor.outputs_eval
        )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Processor: Abstract base class for all dataset processors
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'''
ğŸ§  Every custom data processor must inherit from this class.

It defines the interface Bojai expects for handling data, and automatically handles:
- Storing inputs/outputs
- Train/eval division
- Accessing tokenized and untokenized samples

You MUST implement:
- get_inputs_outputs(): Load your data and return two lists: inputs, outputs
- get_train_eval(): Split those lists into train/eval portions
- __getitem__(): Return a tokenized example
- get_item_untokenized(): Return a raw untokenized example
- __len__(): Return the number of examples

ğŸ“Œ NOTE:
- Inputs/outputs can be any type (text, image paths, numbersâ€¦).
- Tokenization logic is up to you.
'''
class Processor(ABC, Dataset):
    def __init__(self, data_dir, division, model, device, tokenizer, is_main=True, inputs=None, outputs=None):
        super().__init__()

        self.data_dir = data_dir
        self.division = division
        self.tokenizer = tokenizer
        self.model = model
        self.device = device

        if is_main:
            self.inputs, self.outputs = self.get_inputs_outputs(data_dir)
            self.inputs_train, self.inputs_eval, self.outputs_train, self.outputs_eval = self.get_train_eval()
        else:
            self.inputs = inputs
            self.outputs = outputs

    @abstractmethod
    def get_inputs_outputs(self, data_dir):
        '''
        Load your data from the data directory and return:
        - inputs: a list of input samples
        - outputs: a list of output labels (or targets)

        Example:
        return ["Translate this", "Another example"], ["Traduce esto", "Otro ejemplo"]
        '''
        pass

    @abstractmethod
    def get_train_eval(self):
        '''
        Split self.inputs and self.outputs into:
        - inputs_train
        - inputs_eval
        - outputs_train
        - outputs_eval

        Example:
        return inputs[:80], inputs[80:], outputs[:80], outputs[80:]
        '''
        pass

    @abstractmethod
    def __len__(self):
        '''
        Return the number of examples in this dataset
        '''
        pass

    @abstractmethod
    def __getitem__(self, idx):
        '''
        Return a tokenized version of the input/output at the given index

        Example:
        return {
            "input_ids": torch.tensor(...),
            "labels": torch.tensor(...)
        }
        '''
        pass

    @abstractmethod
    def get_item_untokenized(self, idx):
        '''
        Return the raw (untokenized) version of the input/output at the given index

        Example:
        return self.inputs[idx], self.outputs[idx]
        '''
        pass


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# YourDataProcessor: Example implementation of a custom Processor
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class YourDataProcessor(Processor):
    def __init__(self, data_dir, division, model, device, tokenizer, is_main=True, inputs=None, outputs=None):
        super().__init__(data_dir, division, model, device, tokenizer, is_main, inputs, outputs)

    def get_inputs_outputs(self, data_dir):
        # Load your data here â€” for now return empty lists as an example
        return [], []

    def get_train_eval(self):
        # Split your inputs and outputs â€” here we return empty example splits
        return [], [], [], []

    def __len__(self):
        # Number of examples in the dataset
        return len(self.inputs)

    def __getitem__(self, idx):
        # Return tokenized data sample
        return "001011"

    def get_item_untokenized(self, idx):
        # Return untokenized/raw data sample
        return "test"
